{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion_Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTHPYP95cDYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Upgaj4cG37",
        "colab_type": "code",
        "outputId": "c66b5761-c0d9-43cd-b7f7-9c71a21458d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOUwTI8B8J9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "dataset = \"/content/drive/My Drive/neuroscience_project(emotions)/images_new/train_images\"\n",
        "folders = os.listdir(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM-PuXrkCdb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = \"/content/drive/My Drive/neuroscience_project(emotions)/images_new/test_images\"\n",
        "test_folders = os.listdir(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxyY8-7UcOtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/neuroscience_project(emotions)/labels.txt\") as f:\n",
        "    content = f.readlines()\n",
        "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
        "content = [x.strip() for x in content] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZl35owo8dpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Kc=[]\n",
        "for img in content[0:]:\n",
        "    x=img.split(\" \")\n",
        "    Kc.append(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_O6eMsZ8txJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "X_train = []\n",
        "Y_train = []\n",
        "max_numbers = []\n",
        "for img in Kc:\n",
        "\n",
        "    img_name = img[7].replace(\"-\",\".\") + \".\"+img[0]+\".\"+\"tiff\"\n",
        "    \n",
        "    if img_name in folders:\n",
        "        \n",
        "        img_open=cv2.imread(\"/content/drive/My Drive/neuroscience_project(emotions)/images_new/train_images/\"+img_name)\n",
        "\n",
        "        degrees=img[1:7]\n",
        "        \n",
        "        #print(img_name)\n",
        "        \n",
        "        img_open=np.array(img_open)\n",
        "\n",
        "        j=0\n",
        "        for i in degrees:\n",
        "            j+=1\n",
        "            if i==max(degrees):\n",
        "                #print(j-1)\n",
        "                max_numbers.append(j-1)\n",
        "        \n",
        "        #degrees=img[1:7]\n",
        "        \n",
        "        D=[]\n",
        "        \n",
        "        for d in degrees:\n",
        "            D.append(float(d))\n",
        "    \n",
        "        Y_train.append(D)\n",
        "        \n",
        "    \n",
        "        X_train.append(img_open)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxqE7JjAExd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "X_test = []\n",
        "Y_test = []\n",
        "max_numbers_test = []\n",
        "for img in Kc:\n",
        "\n",
        "    img_name = img[7].replace(\"-\",\".\") + \".\"+img[0]+\".\"+\"tiff\"\n",
        "    \n",
        "    if img_name in test_folders:\n",
        "        \n",
        "        img_open=cv2.imread(\"/content/drive/My Drive/neuroscience_project(emotions)/images_new/test_images/\"+img_name)\n",
        "\n",
        "        degrees=img[1:7]\n",
        "        \n",
        "        #print(img_name)\n",
        "        \n",
        "        img_open=np.array(img_open)\n",
        "\n",
        "        j=0\n",
        "        for i in degrees:\n",
        "            j+=1\n",
        "            if i==max(degrees):\n",
        "                #print(j-1)\n",
        "                max_numbers_test.append(j-1)\n",
        "        \n",
        "        #degrees=img[1:7]\n",
        "        \n",
        "        D=[]\n",
        "        \n",
        "        for d in degrees:\n",
        "            D.append(float(d))\n",
        "    \n",
        "        Y_test.append(D)\n",
        "        \n",
        "    \n",
        "        X_test.append(img_open)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqNxSUW7DHl7",
        "colab_type": "text"
      },
      "source": [
        "## Prediction for the degrees for each class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUc0gqiV9jPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = (np.array(X_train)/ 255) - 0.5\n",
        "Y_train = (np.array(Y_train) /5) - 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Z-6jb3GgR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = (np.array(X_test)/ 255) - 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEVPRtqJ9mdk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa011c58-f711-46cc-f079-4873254a334b"
      },
      "source": [
        "from keras.layers import MaxPooling2D, BatchNormalization, Dropout\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, Activation, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "\n",
        "\n",
        "x = Input(shape=(256, 256, 3))#I have just write the right functions and layers as given above from Keras documentation\n",
        "c1 = Conv2D(32, (3, 3), strides=(2, 2),padding='same')(x)\n",
        "b1 = BatchNormalization()(c1)\n",
        "a1 = Activation('relu')(b1)\n",
        "c2 = Conv2D(64, (3, 3), strides=(1, 1),padding='valid')(a1)\n",
        "b2 = BatchNormalization()(c2)\n",
        "a2 = Activation('relu')(b2) \n",
        "p2 = MaxPooling2D(pool_size=(2, 2))(a2)\n",
        "c2 = Conv2D(64, (3, 3), strides=(1, 1),padding='valid')(p2)\n",
        "a2 = Activation('relu')(c2) \n",
        "c2 = Conv2D(32, (3, 3), strides=(2, 2),padding='valid')(a2)\n",
        "d2 = Dropout(0.25)(c2)\n",
        "f2 = Flatten()(d2)\n",
        "\n",
        "h3 = Dense(4000)(f2)\n",
        "b3 = BatchNormalization()(h3)\n",
        "a3 = Activation('relu')(b3)\n",
        "d3 = Dropout(0.5)(a3)\n",
        "a3 = Dense(1000)(d3)\n",
        "b3 = BatchNormalization()(a3)\n",
        "a3 = Activation('relu')(b3)\n",
        "d3 = Dropout(0.5)(a3)\n",
        "z = Dense(100)(a3)\n",
        "b3 = BatchNormalization()(z)\n",
        "a3 = Activation('relu')(b3)\n",
        "d3 = Dropout(0.5)(a3)\n",
        "z = Dense(6)(d3)\n",
        "\n",
        "model = Model(inputs=x, outputs=z)\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
        "plot_model(model, to_file='model.png')\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 126, 126, 64)      18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 126, 126, 64)      256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 126, 126, 64)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 63, 63, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 61, 61, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 61, 61, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 30, 30, 32)        18464     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 28800)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4000)              115204000 \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4000)              16000     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 4000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1000)              4001000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               100100    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 606       \n",
            "=================================================================\n",
            "Total params: 119,401,274\n",
            "Trainable params: 119,390,882\n",
            "Non-trainable params: 10,392\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWdL07cr9uPc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49b1a302-b80e-4e3c-d8b7-c0232f4a0228"
      },
      "source": [
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaP9N9aC91J_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1963b432-f9e7-4362-cfe7-a3e2750028b1"
      },
      "source": [
        "history = model.fit(X_train, Y_train, batch_size=8, epochs=700)#I get %66 when I increase the epoch to 66\n",
        "#model.save('my_model_resolution.h5')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/700\n",
            "179/179 [==============================] - 7s 37ms/step - loss: 1.7524\n",
            "Epoch 2/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 1.0421\n",
            "Epoch 3/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.8510\n",
            "Epoch 4/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.8338\n",
            "Epoch 5/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.7057\n",
            "Epoch 6/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.6535\n",
            "Epoch 7/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.5376\n",
            "Epoch 8/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.5111\n",
            "Epoch 9/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.4751\n",
            "Epoch 10/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.4153\n",
            "Epoch 11/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.3621\n",
            "Epoch 12/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.3108\n",
            "Epoch 13/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.2911\n",
            "Epoch 14/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.2510\n",
            "Epoch 15/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.2550\n",
            "Epoch 16/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.2129\n",
            "Epoch 17/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1996\n",
            "Epoch 18/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1647\n",
            "Epoch 19/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1755\n",
            "Epoch 20/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1508\n",
            "Epoch 21/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1423\n",
            "Epoch 22/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1407\n",
            "Epoch 23/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1453\n",
            "Epoch 24/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1310\n",
            "Epoch 25/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1211\n",
            "Epoch 26/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1134\n",
            "Epoch 27/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1122\n",
            "Epoch 28/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1194\n",
            "Epoch 29/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0997\n",
            "Epoch 30/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0928\n",
            "Epoch 31/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0924\n",
            "Epoch 32/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1002\n",
            "Epoch 33/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.1010\n",
            "Epoch 34/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0870\n",
            "Epoch 35/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0858\n",
            "Epoch 36/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0788\n",
            "Epoch 37/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0809\n",
            "Epoch 38/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0750\n",
            "Epoch 39/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0710\n",
            "Epoch 40/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0654\n",
            "Epoch 41/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0693\n",
            "Epoch 42/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0665\n",
            "Epoch 43/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0656\n",
            "Epoch 44/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0615\n",
            "Epoch 45/700\n",
            "179/179 [==============================] - 2s 8ms/step - loss: 0.0672\n",
            "Epoch 46/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0590\n",
            "Epoch 47/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0577\n",
            "Epoch 48/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0614\n",
            "Epoch 49/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0546\n",
            "Epoch 50/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0543\n",
            "Epoch 51/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0498\n",
            "Epoch 52/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0499\n",
            "Epoch 53/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0520\n",
            "Epoch 54/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0516\n",
            "Epoch 55/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0470\n",
            "Epoch 56/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0473\n",
            "Epoch 57/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0461\n",
            "Epoch 58/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0457\n",
            "Epoch 59/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0530\n",
            "Epoch 60/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0530\n",
            "Epoch 61/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0457\n",
            "Epoch 62/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0436\n",
            "Epoch 63/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0445\n",
            "Epoch 64/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0427\n",
            "Epoch 65/700\n",
            "179/179 [==============================] - 2s 8ms/step - loss: 0.0386\n",
            "Epoch 66/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0438\n",
            "Epoch 67/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0372\n",
            "Epoch 68/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0399\n",
            "Epoch 69/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0428\n",
            "Epoch 70/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0379\n",
            "Epoch 71/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0336\n",
            "Epoch 72/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0361\n",
            "Epoch 73/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0341\n",
            "Epoch 74/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0360\n",
            "Epoch 75/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0373\n",
            "Epoch 76/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0311\n",
            "Epoch 77/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0328\n",
            "Epoch 78/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0364\n",
            "Epoch 79/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0305\n",
            "Epoch 80/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0306\n",
            "Epoch 81/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0302\n",
            "Epoch 82/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0300\n",
            "Epoch 83/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0321\n",
            "Epoch 84/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0335\n",
            "Epoch 85/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0320\n",
            "Epoch 86/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0328\n",
            "Epoch 87/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0297\n",
            "Epoch 88/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0277\n",
            "Epoch 89/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0291\n",
            "Epoch 90/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0284\n",
            "Epoch 91/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0281\n",
            "Epoch 92/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0308\n",
            "Epoch 93/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0297\n",
            "Epoch 94/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0273\n",
            "Epoch 95/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0291\n",
            "Epoch 96/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0279\n",
            "Epoch 97/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0265\n",
            "Epoch 98/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0257\n",
            "Epoch 99/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0264\n",
            "Epoch 100/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0280\n",
            "Epoch 101/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0233\n",
            "Epoch 102/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0251\n",
            "Epoch 103/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0242\n",
            "Epoch 104/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0239\n",
            "Epoch 105/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0236\n",
            "Epoch 106/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0235\n",
            "Epoch 107/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0216\n",
            "Epoch 108/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0233\n",
            "Epoch 109/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0226\n",
            "Epoch 110/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0222\n",
            "Epoch 111/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0222\n",
            "Epoch 112/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0212\n",
            "Epoch 113/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0191\n",
            "Epoch 114/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0215\n",
            "Epoch 115/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0199\n",
            "Epoch 116/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0212\n",
            "Epoch 117/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0214\n",
            "Epoch 118/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0204\n",
            "Epoch 119/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0199\n",
            "Epoch 120/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0211\n",
            "Epoch 121/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0185\n",
            "Epoch 122/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0188\n",
            "Epoch 123/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0188\n",
            "Epoch 124/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0173\n",
            "Epoch 125/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0181\n",
            "Epoch 126/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0169\n",
            "Epoch 127/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0175\n",
            "Epoch 128/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0191\n",
            "Epoch 129/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0169\n",
            "Epoch 130/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0175\n",
            "Epoch 131/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0160\n",
            "Epoch 132/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0173\n",
            "Epoch 133/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0155\n",
            "Epoch 134/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0177\n",
            "Epoch 135/700\n",
            "179/179 [==============================] - 2s 8ms/step - loss: 0.0132\n",
            "Epoch 136/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0127\n",
            "Epoch 137/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0158\n",
            "Epoch 138/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0169\n",
            "Epoch 139/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0143\n",
            "Epoch 140/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0166\n",
            "Epoch 141/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0154\n",
            "Epoch 142/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0140\n",
            "Epoch 143/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0151\n",
            "Epoch 144/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0151\n",
            "Epoch 145/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0155\n",
            "Epoch 146/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0148\n",
            "Epoch 147/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0144\n",
            "Epoch 148/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0160\n",
            "Epoch 149/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0140\n",
            "Epoch 150/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0147\n",
            "Epoch 151/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0184\n",
            "Epoch 152/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0146\n",
            "Epoch 153/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0131\n",
            "Epoch 154/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0152\n",
            "Epoch 155/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0153\n",
            "Epoch 156/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0162\n",
            "Epoch 157/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0133\n",
            "Epoch 158/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0148\n",
            "Epoch 159/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0141\n",
            "Epoch 160/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0157\n",
            "Epoch 161/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0157\n",
            "Epoch 162/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0122\n",
            "Epoch 163/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0151\n",
            "Epoch 164/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0121\n",
            "Epoch 165/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0144\n",
            "Epoch 166/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0121\n",
            "Epoch 167/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0137\n",
            "Epoch 168/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0152\n",
            "Epoch 169/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0141\n",
            "Epoch 170/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0129\n",
            "Epoch 171/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0126\n",
            "Epoch 172/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0152\n",
            "Epoch 173/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0130\n",
            "Epoch 174/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0148\n",
            "Epoch 175/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0143\n",
            "Epoch 176/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0139\n",
            "Epoch 177/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0132\n",
            "Epoch 178/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0133\n",
            "Epoch 179/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0137\n",
            "Epoch 180/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0146\n",
            "Epoch 181/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0138\n",
            "Epoch 182/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0129\n",
            "Epoch 183/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0130\n",
            "Epoch 184/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0127\n",
            "Epoch 185/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0121\n",
            "Epoch 186/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0146\n",
            "Epoch 187/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0140\n",
            "Epoch 188/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0134\n",
            "Epoch 189/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0128\n",
            "Epoch 190/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0125\n",
            "Epoch 191/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0122\n",
            "Epoch 192/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 193/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0124\n",
            "Epoch 194/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0123\n",
            "Epoch 195/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0122\n",
            "Epoch 196/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0116\n",
            "Epoch 197/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0140\n",
            "Epoch 198/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0116\n",
            "Epoch 199/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 200/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0132\n",
            "Epoch 201/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0132\n",
            "Epoch 202/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0122\n",
            "Epoch 203/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 204/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0157\n",
            "Epoch 205/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0146\n",
            "Epoch 206/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0153\n",
            "Epoch 207/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0162\n",
            "Epoch 208/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0163\n",
            "Epoch 209/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 210/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 211/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0120\n",
            "Epoch 212/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 213/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0146\n",
            "Epoch 214/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0120\n",
            "Epoch 215/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0137\n",
            "Epoch 216/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0130\n",
            "Epoch 217/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0122\n",
            "Epoch 218/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0133\n",
            "Epoch 219/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0138\n",
            "Epoch 220/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0136\n",
            "Epoch 221/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0128\n",
            "Epoch 222/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 223/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0123\n",
            "Epoch 224/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0129\n",
            "Epoch 225/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 226/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0130\n",
            "Epoch 227/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0124\n",
            "Epoch 228/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0141\n",
            "Epoch 229/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0126\n",
            "Epoch 230/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 231/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0142\n",
            "Epoch 232/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 233/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0140\n",
            "Epoch 234/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0119\n",
            "Epoch 235/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0140\n",
            "Epoch 236/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 237/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0135\n",
            "Epoch 238/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0136\n",
            "Epoch 239/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0130\n",
            "Epoch 240/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 241/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 242/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0148\n",
            "Epoch 243/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0134\n",
            "Epoch 244/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0126\n",
            "Epoch 245/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0141\n",
            "Epoch 246/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 247/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 248/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 249/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0134\n",
            "Epoch 250/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0139\n",
            "Epoch 251/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 252/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 253/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 254/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0126\n",
            "Epoch 255/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 256/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 257/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0121\n",
            "Epoch 258/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0120\n",
            "Epoch 259/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0129\n",
            "Epoch 260/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 261/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0136\n",
            "Epoch 262/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0126\n",
            "Epoch 263/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 264/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 265/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 266/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 267/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0127\n",
            "Epoch 268/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0140\n",
            "Epoch 269/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 270/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0116\n",
            "Epoch 271/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 272/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 273/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 274/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 275/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 276/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 277/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 278/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0133\n",
            "Epoch 279/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0132\n",
            "Epoch 280/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0126\n",
            "Epoch 281/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 282/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0127\n",
            "Epoch 283/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0119\n",
            "Epoch 284/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 285/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0126\n",
            "Epoch 286/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 287/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0121\n",
            "Epoch 288/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 289/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 290/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 291/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 292/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0123\n",
            "Epoch 293/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 294/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 295/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0122\n",
            "Epoch 296/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0158\n",
            "Epoch 297/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0143\n",
            "Epoch 298/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 299/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 300/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 301/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 302/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0130\n",
            "Epoch 303/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0119\n",
            "Epoch 304/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0144\n",
            "Epoch 305/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 306/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 307/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0120\n",
            "Epoch 308/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0124\n",
            "Epoch 309/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 310/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 311/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 312/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0129\n",
            "Epoch 313/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 314/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0145\n",
            "Epoch 315/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 316/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0125\n",
            "Epoch 317/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 318/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 319/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 320/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 321/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 322/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 323/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 324/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0123\n",
            "Epoch 325/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 326/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 327/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 328/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 329/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 330/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 331/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 332/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0116\n",
            "Epoch 333/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0123\n",
            "Epoch 334/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0101\n",
            "Epoch 335/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0123\n",
            "Epoch 336/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0136\n",
            "Epoch 337/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0121\n",
            "Epoch 338/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 339/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 340/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0098\n",
            "Epoch 341/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 342/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 343/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 344/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 345/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0125\n",
            "Epoch 346/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 347/700\n",
            "179/179 [==============================] - 2s 8ms/step - loss: 0.0134\n",
            "Epoch 348/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 349/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 350/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0099\n",
            "Epoch 351/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0126\n",
            "Epoch 352/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0116\n",
            "Epoch 353/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 354/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 355/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 356/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0098\n",
            "Epoch 357/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0116\n",
            "Epoch 358/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 359/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 360/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0123\n",
            "Epoch 361/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 362/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 363/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 364/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 365/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 366/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 367/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 368/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0124\n",
            "Epoch 369/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0127\n",
            "Epoch 370/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0091\n",
            "Epoch 371/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 372/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 373/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0121\n",
            "Epoch 374/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 375/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 376/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 377/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 378/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 379/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 380/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 381/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0125\n",
            "Epoch 382/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 383/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 384/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 385/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 386/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 387/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 388/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 389/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0101\n",
            "Epoch 390/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 391/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 392/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0131\n",
            "Epoch 393/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 394/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 395/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0123\n",
            "Epoch 396/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 397/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 398/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 399/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0098\n",
            "Epoch 400/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0095\n",
            "Epoch 401/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0124\n",
            "Epoch 402/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 403/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 404/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 405/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 406/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 407/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 408/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0120\n",
            "Epoch 409/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0119\n",
            "Epoch 410/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 411/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 412/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 413/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 414/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 415/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 416/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 417/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 418/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 419/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 420/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 421/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 422/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0101\n",
            "Epoch 423/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0122\n",
            "Epoch 424/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0099\n",
            "Epoch 425/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 426/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 427/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0091\n",
            "Epoch 428/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0120\n",
            "Epoch 429/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 430/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 431/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 432/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 433/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 434/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 435/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 436/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 437/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 438/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 439/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0099\n",
            "Epoch 440/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0090\n",
            "Epoch 441/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0095\n",
            "Epoch 442/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 443/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 444/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 445/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 446/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 447/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 448/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 449/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 450/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0089\n",
            "Epoch 451/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 452/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0082\n",
            "Epoch 453/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 454/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 455/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 456/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 457/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0101\n",
            "Epoch 458/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 459/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 460/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0121\n",
            "Epoch 461/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0141\n",
            "Epoch 462/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 463/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0101\n",
            "Epoch 464/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 465/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 466/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0101\n",
            "Epoch 467/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 468/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 469/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 470/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 471/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 472/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 473/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 474/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 475/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 476/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 477/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 478/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 479/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0089\n",
            "Epoch 480/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 481/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 482/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 483/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 484/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 485/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 486/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 487/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 488/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 489/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0101\n",
            "Epoch 490/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0119\n",
            "Epoch 491/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 492/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 493/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 494/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 495/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0095\n",
            "Epoch 496/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 497/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 498/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0085\n",
            "Epoch 499/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 500/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 501/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 502/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 503/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0099\n",
            "Epoch 504/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0091\n",
            "Epoch 505/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 506/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0090\n",
            "Epoch 507/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 508/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 509/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0095\n",
            "Epoch 510/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 511/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0087\n",
            "Epoch 512/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0087\n",
            "Epoch 513/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 514/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 515/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 516/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 517/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 518/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0080\n",
            "Epoch 519/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 520/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 521/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 522/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 523/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 524/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 525/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 526/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 527/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 528/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 529/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0088\n",
            "Epoch 530/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 531/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 532/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 533/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0118\n",
            "Epoch 534/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0101\n",
            "Epoch 535/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 536/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 537/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 538/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0089\n",
            "Epoch 539/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 540/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0087\n",
            "Epoch 541/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 542/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 543/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 544/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 545/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0110\n",
            "Epoch 546/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0099\n",
            "Epoch 547/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0087\n",
            "Epoch 548/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 549/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 550/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 551/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 552/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 553/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0095\n",
            "Epoch 554/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 555/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0095\n",
            "Epoch 556/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0098\n",
            "Epoch 557/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 558/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0101\n",
            "Epoch 559/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 560/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 561/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0085\n",
            "Epoch 562/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 563/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 564/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 565/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 566/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0095\n",
            "Epoch 567/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0099\n",
            "Epoch 568/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 569/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0090\n",
            "Epoch 570/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 571/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0087\n",
            "Epoch 572/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 573/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0095\n",
            "Epoch 574/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 575/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0090\n",
            "Epoch 576/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 577/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 578/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 579/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 580/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 581/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0082\n",
            "Epoch 582/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 583/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0099\n",
            "Epoch 584/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0088\n",
            "Epoch 585/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 586/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0089\n",
            "Epoch 587/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 588/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 589/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 590/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0091\n",
            "Epoch 591/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0088\n",
            "Epoch 592/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0091\n",
            "Epoch 593/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 594/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0086\n",
            "Epoch 595/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 596/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 597/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 598/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 599/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0085\n",
            "Epoch 600/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 601/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 602/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0086\n",
            "Epoch 603/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0091\n",
            "Epoch 604/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 605/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 606/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 607/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 608/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0115\n",
            "Epoch 609/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 610/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 611/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0111\n",
            "Epoch 612/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0099\n",
            "Epoch 613/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0114\n",
            "Epoch 614/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 615/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0102\n",
            "Epoch 616/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0105\n",
            "Epoch 617/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 618/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 619/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 620/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0081\n",
            "Epoch 621/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0089\n",
            "Epoch 622/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 623/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0107\n",
            "Epoch 624/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 625/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0081\n",
            "Epoch 626/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 627/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 628/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0076\n",
            "Epoch 629/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0087\n",
            "Epoch 630/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 631/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0109\n",
            "Epoch 632/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0089\n",
            "Epoch 633/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0099\n",
            "Epoch 634/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0091\n",
            "Epoch 635/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 636/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0098\n",
            "Epoch 637/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 638/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0090\n",
            "Epoch 639/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0098\n",
            "Epoch 640/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0086\n",
            "Epoch 641/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0104\n",
            "Epoch 642/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 643/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0112\n",
            "Epoch 644/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0095\n",
            "Epoch 645/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0078\n",
            "Epoch 646/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0090\n",
            "Epoch 647/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 648/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0108\n",
            "Epoch 649/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 650/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 651/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 652/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0096\n",
            "Epoch 653/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 654/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0085\n",
            "Epoch 655/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0121\n",
            "Epoch 656/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0085\n",
            "Epoch 657/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 658/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 659/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0090\n",
            "Epoch 660/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0113\n",
            "Epoch 661/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0079\n",
            "Epoch 662/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 663/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0078\n",
            "Epoch 664/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0101\n",
            "Epoch 665/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0082\n",
            "Epoch 666/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0089\n",
            "Epoch 667/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0091\n",
            "Epoch 668/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0075\n",
            "Epoch 669/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0099\n",
            "Epoch 670/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0093\n",
            "Epoch 671/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0080\n",
            "Epoch 672/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0082\n",
            "Epoch 673/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0079\n",
            "Epoch 674/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0089\n",
            "Epoch 675/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0094\n",
            "Epoch 676/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0103\n",
            "Epoch 677/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 678/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 679/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0081\n",
            "Epoch 680/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0086\n",
            "Epoch 681/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0090\n",
            "Epoch 682/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0082\n",
            "Epoch 683/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 684/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0100\n",
            "Epoch 685/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0117\n",
            "Epoch 686/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0089\n",
            "Epoch 687/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0078\n",
            "Epoch 688/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0097\n",
            "Epoch 689/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0078\n",
            "Epoch 690/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0083\n",
            "Epoch 691/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0095\n",
            "Epoch 692/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0098\n",
            "Epoch 693/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0087\n",
            "Epoch 694/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0091\n",
            "Epoch 695/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 696/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0106\n",
            "Epoch 697/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0084\n",
            "Epoch 698/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0081\n",
            "Epoch 699/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0092\n",
            "Epoch 700/700\n",
            "179/179 [==============================] - 1s 8ms/step - loss: 0.0079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUN7hDrM-Zl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=model.predict(X_test[0:33])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AB_EpNu-in2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "d255ed95-eb7d-433b-9512-cbe6d4d1cec8"
      },
      "source": [
        "Y_pred=(x + 0.5)*5\n",
        "Y_pred"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.2342343, 2.068236 , 1.835964 , 1.7762984, 1.7388678, 1.7229973],\n",
              "       [1.5587085, 3.6228297, 1.9137825, 2.5832727, 3.2662158, 2.7441475],\n",
              "       [1.9292421, 2.734121 , 1.7730862, 3.160236 , 3.1133568, 2.163347 ],\n",
              "       [1.760988 , 2.7104702, 3.4529614, 2.732551 , 3.6575139, 3.3502426],\n",
              "       [1.4844502, 3.6829681, 2.1052375, 2.3336163, 3.441229 , 3.068774 ],\n",
              "       [1.4288689, 2.2053885, 1.8614588, 3.7605977, 4.032261 , 2.0940537],\n",
              "       [2.2166169, 2.133165 , 4.095239 , 2.0219657, 2.264574 , 2.4880989],\n",
              "       [2.3534882, 2.7643814, 2.1594605, 2.0830674, 2.343261 , 2.2804084],\n",
              "       [1.472339 , 4.431842 , 1.556519 , 1.8893123, 2.4960508, 2.5415134],\n",
              "       [2.3512883, 1.9714019, 4.316474 , 1.6981785, 1.9040166, 2.118909 ],\n",
              "       [1.5126525, 2.4243393, 2.0427046, 4.017758 , 3.197625 , 2.0443993],\n",
              "       [1.882896 , 3.2338712, 2.8811088, 2.072301 , 2.6454344, 3.1421754],\n",
              "       [1.5215738, 3.6393323, 3.6138442, 2.0165746, 2.9329154, 3.9484322],\n",
              "       [1.9125438, 3.1858706, 1.9788532, 2.4680696, 2.9075143, 2.46425  ],\n",
              "       [2.1557312, 2.2120643, 3.681898 , 2.1658816, 2.2348   , 2.3557694],\n",
              "       [2.0554194, 2.3999877, 3.4623897, 2.201273 , 2.2609134, 2.3466153],\n",
              "       [2.968711 , 2.2029963, 2.133019 , 1.9453368, 2.017331 , 1.8791604],\n",
              "       [2.1636558, 2.6115348, 1.9512194, 2.5203424, 2.9345086, 2.3217125],\n",
              "       [2.5726204, 2.3789475, 2.1869278, 2.1413531, 2.2601254, 2.092768 ],\n",
              "       [1.4691231, 2.981505 , 2.041197 , 3.0424893, 4.0685325, 2.7541847],\n",
              "       [1.4339268, 3.4004903, 3.8515282, 2.276513 , 3.405607 , 4.053623 ],\n",
              "       [1.5689164, 3.0184948, 2.023375 , 3.1404672, 3.6703167, 2.5979486],\n",
              "       [2.0850587, 2.4929528, 2.958159 , 2.352621 , 2.4274259, 2.4100683],\n",
              "       [2.1395311, 2.5332866, 2.6346123, 2.3258355, 2.4030612, 2.31698  ],\n",
              "       [1.3755345, 3.8373995, 2.1351771, 2.753196 , 3.4665518, 3.060265 ],\n",
              "       [1.5535312, 3.220469 , 2.2080894, 2.8782158, 3.6730103, 2.7779474],\n",
              "       [1.9486525, 2.321827 , 4.2152414, 2.1471167, 2.2459233, 2.5887344],\n",
              "       [3.1776962, 2.093628 , 1.9767914, 1.803205 , 1.814983 , 1.7688484],\n",
              "       [1.8002104, 2.6735094, 3.9836895, 2.1796794, 2.494051 , 2.8911278],\n",
              "       [1.8250726, 2.557569 , 3.9264102, 2.2717493, 2.5018976, 2.758012 ],\n",
              "       [2.5260196, 2.3967884, 2.0749867, 2.2874105, 2.4209483, 2.1552544],\n",
              "       [4.0964594, 1.6416023, 1.6648144, 1.6169494, 1.662926 , 1.5490546],\n",
              "       [4.234569 , 1.5650508, 1.6449902, 1.5838796, 1.5689821, 1.4881194]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9EhCWOD_Ffv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "2933ebcb-a101-47b7-c50b-776eaab9ef9d"
      },
      "source": [
        "Y_test[0:33]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2.87, 2.42, 1.58, 1.84, 1.77, 1.77],\n",
              " [1.23, 4.39, 1.45, 2.61, 3.19, 2.71],\n",
              " [1.39, 2.03, 1.57, 4.58, 3.77, 1.68],\n",
              " [1.5, 2.97, 3.67, 2.83, 3.77, 3.8],\n",
              " [1.26, 4.13, 1.77, 2.4, 3.1, 2.74],\n",
              " [1.67, 1.97, 1.7, 4.6, 4.1, 1.77],\n",
              " [1.39, 3.0, 4.29, 2.32, 3.74, 4.13],\n",
              " [3.03, 2.45, 1.74, 2.0, 1.9, 1.77],\n",
              " [1.65, 4.26, 1.77, 1.94, 2.61, 2.77],\n",
              " [2.35, 1.81, 4.87, 1.81, 1.71, 2.29],\n",
              " [1.19, 2.16, 1.55, 4.74, 2.84, 1.39],\n",
              " [1.29, 3.52, 3.48, 2.03, 3.42, 3.35],\n",
              " [1.29, 3.13, 4.52, 2.55, 3.45, 4.19],\n",
              " [4.45, 1.29, 1.1, 1.19, 1.26, 1.19],\n",
              " [2.0, 1.77, 4.68, 1.81, 1.65, 2.03],\n",
              " [2.42, 1.84, 4.74, 1.94, 1.87, 2.06],\n",
              " [3.03, 2.32, 1.77, 1.74, 1.77, 1.68],\n",
              " [2.03, 2.29, 1.61, 2.71, 2.35, 1.87],\n",
              " [3.48, 2.19, 1.55, 1.68, 1.55, 1.58],\n",
              " [1.61, 2.52, 2.29, 2.68, 4.71, 3.19],\n",
              " [1.23, 4.06, 4.1, 2.32, 3.26, 4.26],\n",
              " [1.45, 2.35, 1.58, 4.23, 3.1, 1.81],\n",
              " [1.84, 2.29, 3.71, 2.35, 2.32, 2.13],\n",
              " [2.1, 2.13, 3.61, 2.16, 2.1, 2.06],\n",
              " [1.26, 4.26, 1.9, 2.87, 3.61, 3.35],\n",
              " [1.39, 4.42, 1.94, 2.77, 3.68, 3.81],\n",
              " [1.84, 1.8, 4.84, 2.16, 1.9, 2.19],\n",
              " [2.97, 1.9, 1.42, 1.61, 1.52, 1.32],\n",
              " [1.65, 2.77, 4.61, 2.1, 2.87, 3.74],\n",
              " [2.03, 2.03, 4.58, 1.87, 1.94, 2.81],\n",
              " [2.68, 2.1, 1.61, 2.19, 2.03, 1.68],\n",
              " [4.77, 1.35, 1.42, 1.13, 1.13, 1.16],\n",
              " [4.48, 1.42, 1.48, 1.32, 1.55, 1.35]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBqhbRnRDU7K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2192cc6c-c861-4489-b2e0-aa8691e757a9"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# x-axis values \n",
        "x = [ ]\n",
        "\n",
        "for i in range(33):\n",
        "  x.append(i)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i in Y_pred:\n",
        "  predictions.append(i[0])\n",
        "\n",
        "ground_truth = []\n",
        "\n",
        "for j in Y_test:\n",
        "  ground_truth.append(j[0])\n",
        "\n",
        "\n",
        "# plotting points as a scatter plot \n",
        "plt.scatter(x, predictions, label= \"prediction\", color= \"green\", \n",
        "\t\t\tmarker= \"o\", s=30) \n",
        "plt.scatter(x, ground_truth, label= \"true-value\", color= \"red\", \n",
        "\t\t\tmarker= \"o\", s=30) \n",
        "# x-axis label \n",
        "plt.xlabel('test-images') \n",
        "# frequency label \n",
        "plt.ylabel('degree(1-5)') \n",
        "# plot title \n",
        "plt.title('Fear') \n",
        "# showing legend \n",
        "plt.legend() \n",
        "\n",
        "# function to show the plot \n",
        "plt.show() \n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc/8fCzk2rMGgwH0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsUVmsrBMp2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67ecf4cd-2b3f-4867-f1e8-5d2b3d455e22"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(Y_test,Y_pred)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.30172630668345735"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5HwD4IIC4AW",
        "colab_type": "text"
      },
      "source": [
        "## ALEX NET() - Classification based on maximum class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMWFTYoaNG-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_class = []\n",
        "for i in max_numbers:\n",
        "  class_num = [0,0,0,0,0,0]\n",
        "  class_num[i] = 1\n",
        "  Y_train_class.append(class_num)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8m164CGOBxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test_class = []\n",
        "for i in max_numbers_test:\n",
        "  class_num = [0,0,0,0,0,0]\n",
        "  class_num[i] = 1\n",
        "  Y_test_class.append(class_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qDNPFP6Rm68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f323fa04-c7cb-4fdb-eaea-a90df6b72f05"
      },
      "source": [
        "len(Y_test_class)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X-Z0AzmC3Qb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ffb80f2-2ef6-48d3-e2ba-b9b366f52439"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "#np.random.seed(1000)\n",
        "#Instantiate an empty model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96, input_shape=(256,256,3), kernel_size=(11,11), strides=(4,4), padding=\"valid\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\"))\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding=\"valid\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\"))\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\"valid\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\"valid\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding=\"valid\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\"))\n",
        "\n",
        "# Passing it to a Fully Connected layer\n",
        "model.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation(\"relu\"))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# 2nd Fully Connected Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation(\"relu\"))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# 3rd Fully Connected Layer\n",
        "model.add(Dense(1000))\n",
        "model.add(Activation(\"relu\"))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(6))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 62, 62, 96)        34944     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 62, 62, 96)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 31, 31, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 21, 21, 256)       2973952   \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 21, 21, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 10, 10, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 384)         885120    \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 8, 8, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 6, 6, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 4, 4, 256)         884992    \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4096)              4198400   \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 6)                 6006      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 31,189,214\n",
            "Trainable params: 31,189,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEY6kNAuSWYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_class = np.array(Y_train_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvFwfLH4SAPG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "outputId": "36933b24-a184-4a93-a0b1-c46b93b9eab5"
      },
      "source": [
        "history = model.fit(X_train, Y_train_class, batch_size=8, epochs=50)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "179/179 [==============================] - 3s 14ms/step - loss: 12.2359 - acc: 0.2179\n",
            "Epoch 2/50\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 12.5163 - acc: 0.2235\n",
            "Epoch 3/50\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 12.5163 - acc: 0.2235\n",
            "Epoch 4/50\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 12.5163 - acc: 0.2235\n",
            "Epoch 5/50\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 12.5163 - acc: 0.2235\n",
            "Epoch 6/50\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 12.5163 - acc: 0.2235\n",
            "Epoch 7/50\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 12.5163 - acc: 0.2235\n",
            "Epoch 8/50\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 12.5163 - acc: 0.2235\n",
            "Epoch 9/50\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 12.5163 - acc: 0.2235\n",
            "Epoch 10/50\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 12.5163 - acc: 0.2235\n",
            "Epoch 11/50\n",
            "  8/179 [>.............................] - ETA: 0s - loss: 12.0886 - acc: 0.2500"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-cddbfb012005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# For backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# wait for flush to actually get through:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0mevt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mevent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HX0sUe1TGH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "outputId": "8afe7f9a-83e9-40d9-f10d-fe6e7b71a4af"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import optimizers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(64, 3,3,border_mode='same',input_shape=(256,256,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Convolution2D(128, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(128, 3, 3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(6))\n",
        "model.add(Activation('softmax'))\n",
        "adam = keras.optimizers.Adam(lr=0.5)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "model.get_config()\n",
        "model.layers[0].get_config()\n",
        "model.layers[0].input_shape\n",
        "model.layers[0].output_shape\n",
        "model.layers[0].get_weights()\n",
        "np.shape(model.layers[0].get_weights()[0])\n",
        "model.layers[0].trainable"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), input_shape=(256, 256,..., padding=\"same\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 256, 256, 64)      1792      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 256, 256, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 254, 254, 64)      36928     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 254, 254, 64)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 127, 127, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 125, 125, 128)     73856     \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 125, 125, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 123, 123, 128)     147584    \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 123, 123, 128)     0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 61, 61, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 61, 61, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 476288)            0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                30482496  \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 6)                 390       \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 30,743,046\n",
            "Trainable params: 30,743,046\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c51o_gi9TM-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73ec1bed-0cb2-4791-df69-0f0afa0bc4c1"
      },
      "source": [
        "history = model.fit(X_train, Y_train_class, batch_size=8, epochs=50)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "179/179 [==============================] - 3s 16ms/step - loss: 2.3168 - acc: 0.2123\n",
            "Epoch 2/50\n",
            "179/179 [==============================] - 2s 12ms/step - loss: 1.7531 - acc: 0.2346\n",
            "Epoch 3/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 1.6601 - acc: 0.3296\n",
            "Epoch 4/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 1.4624 - acc: 0.5140\n",
            "Epoch 5/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 1.0808 - acc: 0.6536\n",
            "Epoch 6/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.8489 - acc: 0.7374\n",
            "Epoch 7/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.6386 - acc: 0.7821\n",
            "Epoch 8/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.4656 - acc: 0.8380\n",
            "Epoch 9/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.2632 - acc: 0.8939\n",
            "Epoch 10/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.2069 - acc: 0.9385\n",
            "Epoch 11/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.1745 - acc: 0.9385\n",
            "Epoch 12/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.1267 - acc: 0.9441\n",
            "Epoch 13/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.1170 - acc: 0.9721\n",
            "Epoch 14/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0722 - acc: 0.9721\n",
            "Epoch 15/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.1004 - acc: 0.9665\n",
            "Epoch 16/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0170 - acc: 0.9944\n",
            "Epoch 17/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0925 - acc: 0.9721\n",
            "Epoch 18/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0120 - acc: 0.9888\n",
            "Epoch 19/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0649 - acc: 0.9832\n",
            "Epoch 20/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0147 - acc: 0.9888\n",
            "Epoch 21/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 22/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0623 - acc: 0.9888\n",
            "Epoch 23/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 24/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 9.5007e-05 - acc: 1.0000\n",
            "Epoch 25/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0320 - acc: 0.9944\n",
            "Epoch 26/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0563 - acc: 0.9888\n",
            "Epoch 27/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 28/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 1.1326e-04 - acc: 1.0000\n",
            "Epoch 29/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 2.6640e-06 - acc: 1.0000\n",
            "Epoch 30/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 3.6024e-05 - acc: 1.0000\n",
            "Epoch 31/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 4.2323e-07 - acc: 1.0000\n",
            "Epoch 32/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 2.0212e-07 - acc: 1.0000\n",
            "Epoch 33/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 1.7282e-07 - acc: 1.0000\n",
            "Epoch 34/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.2761 - acc: 0.9777\n",
            "Epoch 35/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 0.0651 - acc: 0.9888\n",
            "Epoch 36/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 1.5976e-05 - acc: 1.0000\n",
            "Epoch 37/50\n",
            "179/179 [==============================] - 2s 12ms/step - loss: 6.1703e-07 - acc: 1.0000\n",
            "Epoch 38/50\n",
            "179/179 [==============================] - 2s 12ms/step - loss: 1.4923e-05 - acc: 1.0000\n",
            "Epoch 39/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 6.2735e-07 - acc: 1.0000\n",
            "Epoch 40/50\n",
            "179/179 [==============================] - 2s 12ms/step - loss: 1.6549e-07 - acc: 1.0000\n",
            "Epoch 41/50\n",
            "179/179 [==============================] - 2s 12ms/step - loss: 1.1325e-06 - acc: 1.0000\n",
            "Epoch 42/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 1.5084e-07 - acc: 1.0000\n",
            "Epoch 43/50\n",
            "179/179 [==============================] - 2s 12ms/step - loss: 1.2154e-07 - acc: 1.0000\n",
            "Epoch 44/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 1.2520e-07 - acc: 1.0000\n",
            "Epoch 45/50\n",
            "179/179 [==============================] - 2s 12ms/step - loss: 1.2021e-07 - acc: 1.0000\n",
            "Epoch 46/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 1.1988e-07 - acc: 1.0000\n",
            "Epoch 47/50\n",
            "179/179 [==============================] - 2s 12ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
            "Epoch 48/50\n",
            "179/179 [==============================] - 2s 12ms/step - loss: 1.1921e-07 - acc: 1.0000\n",
            "Epoch 49/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 6.5732e-07 - acc: 1.0000\n",
            "Epoch 50/50\n",
            "179/179 [==============================] - 2s 13ms/step - loss: 1.1921e-07 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbjuXCiDVj_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=model.predict(X_test[0:33])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErHcGVXsVyXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k=np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xUpc27bWOqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJEciviSWkJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a2777ad-fb19-4ecf-8c19-bcd798c8e4e1"
      },
      "source": [
        "accuracy_score(max_numbers_test,k)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6363636363636364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}